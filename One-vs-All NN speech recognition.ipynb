{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "from python_speech_features import mfcc, logfbank\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "features_df = pd.read_csv('data.csv')\n",
    "\n",
    "# get train label and data\n",
    "all_labels = features_df.values[:,0]\n",
    "all_labels=all_labels[:]+1\n",
    "x_data = features_df.values[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0}\n"
     ]
    }
   ],
   "source": [
    "# get unique labesl\n",
    "unique_words = set(all_labels)\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training matrix: (18217, 2808)\n",
      "Size of testing matrix: (3215, 2808)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=0)\n",
    "\n",
    "for train_index, test_index in sss.split(x_data,all_labels):\n",
    "    X_train, X_test = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_test = all_labels[train_index], all_labels[test_index]\n",
    "    \n",
    "\n",
    "print('Size of training matrix:', X_train.shape)\n",
    "print('Size of testing matrix:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize train\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "\n",
    "# normalize test\n",
    "scaler = preprocessing.StandardScaler().fit(X_test)\n",
    "X_test=scaler.transform(X_test)\n",
    "\n",
    "#get number of columns in training data\n",
    "n_cols = x_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMBALANCED DATASET\n",
    "In this document we train our model by utilizing one vs all apprach. Thefore the data of single label (ones) is considerably low than that of others. Before train our model, we need to fix the imbalance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw chart\n",
    "def pie_chart(y,label):\n",
    "    one_shape = y[y==label].shape[0]\n",
    "    others_shape = y[y!=label].shape[0]\n",
    "    plt.pie(\n",
    "        list([one_shape, others_shape]),\n",
    "        labels=['Label {}: {}'.format(label,one_shape),'{}: {}'.format('others',others_shape)]\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize imbalanced dataset: exmaple for label 1\n",
    "pie_chart(y_train,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE\n",
    "SMOTE (Synthetic Minority Over-Sampling Technique) is an over-sampling technique that introduces small perturbations to synthetic examples along the direction of existing samples to reduce overfitting. See original paper for detailed explanation of SMOTE.\n",
    "\n",
    "\n",
    "#### SMOTE Implementation\n",
    "\n",
    "There is a SMOTE implementation in imblearn package for scikit-learn. However, there is not an option to apply SMOTE with arbitrary percentages (SMOTE-100, SMOTE-300, etc.); it simply balances all the classes. And also since SMOTE is not a hard to implement algorithm, we provide our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def smote(samples, amount, k=5):\n",
    "    \"\"\"\n",
    "    Apply SMOTE algorithm to samples and return a new samples\n",
    "    array with synthetically created samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    samples: (n_samples, n_features) samples array to be sent to\n",
    "             SMOTE algorithm.\n",
    "    amount: Percentage of newly created synthetic samples. (E.g.\n",
    "            amount=100 would create as many synthetic examples\n",
    "            as existing ones).\n",
    "    k: Number of nearest neighbors in SMOTE algorithm.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out: ((1 + amount/100)*n_samples, n_features) samples array containing\n",
    "         the original and the newly created synthetic examples.\n",
    "         \n",
    "    References\n",
    "    ----------\n",
    "    http://www.jair.org/media/953/live-953-2037-jair.pdf\n",
    "    \"\"\"\n",
    "    samples = np.copy(samples)\n",
    "    n_samples, n_features = samples.shape\n",
    "    # handle amount < 100 case\n",
    "    if amount < 100:\n",
    "        num_samples = int(len(samples)*(amount/100))\n",
    "        np.shuffle(samples)\n",
    "        samples = samples[:num_samples, :]\n",
    "        amount = 100\n",
    "    amount = int(amount/100)\n",
    "    synthetic = np.empty((n_samples*amount, n_features))\n",
    "    # find k nearest neighbors of each point and store it in nnarray\n",
    "    nbrs = NearestNeighbors(n_neighbors=k + 1).fit(samples)\n",
    "    _, nnarray = nbrs.kneighbors(samples)\n",
    "    nnarray = nnarray[:, 1:]  # get rid of self-nearest-neighbor.\n",
    "    # create synthetic examples and store them in synthetic.\n",
    "    for i, neighbors in enumerate(nnarray):\n",
    "        for j in range(amount):\n",
    "            chosen = neighbors[randint(0, k - 1)]\n",
    "            diff = samples[chosen] - samples[i]\n",
    "            gap = np.random.rand(n_features)\n",
    "            synthetic[i*amount + j] = samples[i] + gap*diff\n",
    "    out = np.vstack((samples, synthetic))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Synthetic SMOTE Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample array\n",
    "arr1 = np.random.normal(loc=5, scale=2.5, size=(50, 2))\n",
    "arr2 = np.random.normal(loc=0, scale=2.5, size=(20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arr_smote = smote(arr2, 100)\n",
    "plt.scatter(*arr1.T, label='Majority')\n",
    "plt.scatter(*arr2.T, label='Minority')\n",
    "plt.scatter(*arr_smote[25:, :].T, label='new SMOTE samples')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_sampling(X, y, label):\n",
    "\n",
    "    other_n = y[y!=label].shape[0]\n",
    "    one_n = y[y==label].shape[0]\n",
    "    smote_amount = int(((other_n-one_n)*100)/one_n)\n",
    "\n",
    "    X_one = X[y==label]\n",
    "    synthetic_data = smote(X_one, smote_amount)\n",
    "    n_synthetic = len(synthetic_data)\n",
    "\n",
    "    # merge synthetic examples with original examples\n",
    "    X_out = np.vstack((X[y!=label], synthetic_data))\n",
    "    y_out = np.concatenate((y[y!=label], [label]*n_synthetic))\n",
    "\n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example for label 1 after upsampling\n",
    "X_out, y_out = smote_sampling(X_train, y_train, 1)\n",
    "pie_chart(y_out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ronadom_undersample(X,y,label):\n",
    "    X1 = X[y==label]\n",
    "    y1 = y[y==label]\n",
    "\n",
    "    # define others labels\n",
    "    # and reduce theri elements randomly\n",
    "    X2 = X[y!=label]\n",
    "    y2 = y[y!=label]\n",
    "    idx = np.random.choice(np.arange(X2.shape[0]), int(X1.shape[0]*4), replace=False)\n",
    "    X2_new = X2[idx,:]\n",
    "    y2_new = y2[idx]\n",
    "\n",
    "    X_out = np.vstack((X1, X2_new))\n",
    "    y_out = np.concatenate((y1, y2_new))\n",
    "\n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example for label 1 after undersampling\n",
    "X_out, y_out = ronadom_undersample(X_train, y_train, 1)\n",
    "pie_chart(y_out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_oversmaple(X,y,label):\n",
    "    \n",
    "    X1 = X[y==label]\n",
    "    y1 = y[y==label]\n",
    "\n",
    "    # define others labels\n",
    "    # and reduce theri elements randomly\n",
    "    X2 = X[y!=label]\n",
    "    y2 = y[y!=label]\n",
    "    \n",
    "    X1_new=[X1]*int(X2.shape[0]/X1.shape[0])\n",
    "    X1_new=np.array(X1_new)\n",
    "    X1_new=np.resize(X1_new,(X1_new.shape[0]*X1_new.shape[1],X2.shape[1]))\n",
    "    \n",
    "    y1_new=[y1]*int(y2.shape[0]/y1.shape[0])\n",
    "    y1_new=np.array(y1_new)\n",
    "    y1_new=np.resize(y1_new,(y1_new.shape[0]*y1_new.shape[1]))\n",
    "                     \n",
    "                     \n",
    "    X_out = np.vstack((X1_new, X2))\n",
    "    y_out = np.concatenate((y1_new, y2))\n",
    "\n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to handle NN processing\n",
    "class NNTrainer(object):  \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    layer_activation: activation function for input and hidden layers\n",
    "    covariance_type: activation function of end layer\n",
    "    input_n_cols: numver of columns of input layer\n",
    "    optimizer_function: optimization function\n",
    "    loss_function: loss functions\n",
    "    metrics_v: metric for evaluation result\n",
    "    epochs_n: number of epoches to update train weights\n",
    "    batch_size_n: batch size of fitted data\n",
    "    validation_split_n: ratio of validation split in traning \n",
    "    \n",
    "    choice of parameters depends on the data. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, layer_activation='relu', end_layer_activation='softmax',input_n_cols=2808,\n",
    "                 optimizer_function='adam',loss_function='categorical_crossentropy',metrics_v='accuracy',\n",
    "                epochs_n=20, batch_size_n=20, validation_split_n=0.2):\n",
    "        \n",
    "        # initialize variables\n",
    "        self.layer_activation = layer_activation\n",
    "        self.end_layer_activation = end_layer_activation\n",
    "        self.input_n_cols = input_n_cols\n",
    "        self.optimizer_function = optimizer_function\n",
    "        self.loss_function = loss_function\n",
    "        self.metrics_v = metrics_v\n",
    "        self.epochs_n = epochs_n\n",
    "        self.batch_size_n =batch_size_n\n",
    "        self.validation_split_n = validation_split_n\n",
    "        \n",
    "        \n",
    "        # define model\n",
    "        self.model = Sequential()\n",
    "        #add layers to model and initialize\n",
    "        self.model.add(Dense(200, activation=self.layer_activation, input_shape=(self.input_n_cols,)))\n",
    "        self.model.add(Dense(200, activation=self.layer_activation, input_shape=(self.input_n_cols,)))\n",
    "        self.model.add(Dense(200, activation=self.layer_activation))\n",
    "#         self.model.add(Dropout(0.2))\n",
    "#         self.model.add(Dense(50, activation=self.layer_activation))\n",
    "        self.model.add(Dense(2, activation=self.end_layer_activation))\n",
    "        \n",
    "        # compile model\n",
    "        self.model.compile(optimizer=self.optimizer_function, \n",
    "                           loss=self.loss_function,metrics=[self.metrics_v])\n",
    "            \n",
    "    #train mode\n",
    "    def train(self, X_train, y_train):\n",
    "        # ingonre divisin by 0\n",
    "        # np.seterr(all='ignore') \n",
    "        #train model\n",
    "        self.model.fit(X_train, y_train, epochs=self.epochs_n,\n",
    "                       batch_size=self.batch_size_n,validation_split=self.validation_split_n)\n",
    "         \n",
    "    # run the model on new data and get score\n",
    "    def predict_probability(self, X_test):\n",
    "        scores = self.model.predict_proba(X_test)\n",
    "        return scores\n",
    "\n",
    "    \n",
    "    # return model\n",
    "    def model_evaluate(self, X_test, y_test):\n",
    "        scores = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        return scores[1]*100\n",
    "    \n",
    "    # return model\n",
    "    def get_nn_model():\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN MODELS\n",
    "\n",
    "Train each model seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_t_data=[]\n",
    "# y_t_data=[]\n",
    "\n",
    "# for label in unique_words:\n",
    "    \n",
    "#     # random undersampling \n",
    "#     # X_out, y_out = ronadom_undersample(X_train, y_train, int(label))\n",
    "    \n",
    "#     # oversmapling smote\n",
    "#     # X_out,y_out = smote_sampling(X_train, y_train, int(label))\n",
    "    \n",
    "#     print(label)\n",
    "    \n",
    "#     # oversampling multiply\n",
    "#     X_out, y_out = multiply_oversmaple(X_train, y_train, int(label))\n",
    "    \n",
    "#     # shullfe data\n",
    "    \n",
    "#     time.sleep(30)\n",
    "    \n",
    "#     print(\"shuffle\")\n",
    "    \n",
    "#     X_out, y_out = shuffle(X_out, y_out, random_state=0)\n",
    "    \n",
    "#     X_t_data.insert(int(label)-1,X_out)\n",
    "#     y_t_data.insert(int(label)-1,y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 951us/step - loss: 0.0157 - acc: 0.9942 - val_loss: 0.0183 - val_acc: 0.9978\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 13s 878us/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0116 - val_acc: 0.9970\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 13s 879us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0078 - val_acc: 0.9992\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 2.5293e-06 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 0.9992\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 840us/step - loss: 1.3064e-06 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 0.9992\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 8.0251e-07 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 0.9992\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 5.3105e-07 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 0.9992\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 3.7631e-07 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 0.9992\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 2.8104e-07 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 2.2168e-07 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 840us/step - loss: 1.8455e-07 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 0.9992\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 1.6072e-07 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9992\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 837us/step - loss: 1.4569e-07 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9992c\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 1.3595e-07 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 0.9992\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 848us/step - loss: 1.2950e-07 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 0.9992\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 1.2563e-07 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 1.2314e-07 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 1.2164e-07 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 848us/step - loss: 1.2068e-07 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 0.9992\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 852us/step - loss: 1.2006e-07 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 0.9992\n",
      "2\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 936us/step - loss: 0.0365 - acc: 0.9927 - val_loss: 0.0851 - val_acc: 0.9866\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 856us/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0035 - val_acc: 0.9989\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 13s 892us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 0.0062 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 13s 868us/step - loss: 0.0042 - acc: 0.9994 - val_loss: 2.5674e-05 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 855us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 1.1954e-04 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 13s 886us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 9.6278e-05 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 13s 910us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 7.6397e-05 - val_acc: 1.0000.9\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 13s 871us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 5.9972e-05 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 858us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 4.6919e-05 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 855us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 3.6924e-05 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 856us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.9518e-05 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.3424e-05 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 861us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 1.8496e-05 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 1.4384e-05 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 1.1276e-05 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 853us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 8.6076e-06 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 6.7335e-06 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 852us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 5.5308e-06 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 852us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 4.4729e-06 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 3.6306e-06 - val_acc: 1.0000\n",
      "3\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 13s 921us/step - loss: 0.0176 - acc: 0.9945 - val_loss: 0.0099 - val_acc: 0.9984\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0096 - acc: 0.9977 - val_loss: 0.0030 - val_acc: 0.9992\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0037 - acc: 0.9990 - val_loss: 8.8998e-04 - val_acc: 0.9997\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0018 - val_acc: 0.9997\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 8.9057e-04 - val_acc: 0.9997\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 853us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 9.5170e-04 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 13s 866us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 9.6562e-04 - val_acc: 0.9997\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 14s 933us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0010 - val_acc: 0.9997\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 14s 960us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0010 - val_acc: 0.9997\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 852us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0011 - val_acc: 0.9997\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0011 - val_acc: 0.9997\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0012 - val_acc: 0.9997\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0012 - val_acc: 0.9997s - loss: 0.0012\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 841us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0013 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 841us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0014 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 861us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 13s 866us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0015 - val_acc: 0.9997\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0016 - val_acc: 0.9997\n",
      "4\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 934us/step - loss: 0.0257 - acc: 0.9922 - val_loss: 0.0130 - val_acc: 0.9984\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 13s 864us/step - loss: 0.0079 - acc: 0.9981 - val_loss: 0.0091 - val_acc: 0.9986\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 13s 864us/step - loss: 0.0039 - acc: 0.9990 - val_loss: 0.0193 - val_acc: 0.9951\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 13s 868us/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0143 - val_acc: 0.9975\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 13s 864us/step - loss: 0.0021 - acc: 0.9997 - val_loss: 0.0099 - val_acc: 0.9984\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 13s 866us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0114 - val_acc: 0.9981\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 13s 874us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0116 - val_acc: 0.9981\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 13s 876us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0118 - val_acc: 0.9981\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 866us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0120 - val_acc: 0.9981\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 13s 893us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0123 - val_acc: 0.9981\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 13s 876us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0125 - val_acc: 0.9981\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 13s 871us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0127 - val_acc: 0.9981\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 866us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0129 - val_acc: 0.9981\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 14s 980us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0131 - val_acc: 0.9981\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 14s 968us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0132 - val_acc: 0.9981\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 904us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0133 - val_acc: 0.9981\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 885us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0134 - val_acc: 0.9981\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 13s 877us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0135 - val_acc: 0.9981\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 13s 877us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0136 - val_acc: 0.9984\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 877us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0137 - val_acc: 0.9984\n",
      "5\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 990us/step - loss: 0.0160 - acc: 0.9955 - val_loss: 0.0037 - val_acc: 0.9989\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 13s 902us/step - loss: 0.0063 - acc: 0.9992 - val_loss: 0.0116 - val_acc: 0.9981\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 14s 958us/step - loss: 0.0088 - acc: 0.9987 - val_loss: 8.0309e-04 - val_acc: 0.9995loss: 0.0089 - acc: 0.\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 14s 940us/step - loss: 0.0037 - acc: 0.9995 - val_loss: 0.0098 - val_acc: 0.9989\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0034 - acc: 0.9998 - val_loss: 0.0139 - val_acc: 0.9984\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0038 - acc: 0.9997 - val_loss: 0.0079 - val_acc: 0.9989\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0057 - acc: 0.9996 - val_loss: 0.0126 - val_acc: 0.9989\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 14s 947us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0123 - val_acc: 0.9992\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 909us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0123 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0122 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 14s 975us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0122 - val_acc: 0.9992\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0122 - val_acc: 0.9992\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0122 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0122 - val_acc: 0.9992\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 14s 946us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0123 - val_acc: 0.9992\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0123 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 14s 971us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0123 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 14s 961us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0123 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 13s 926us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0124 - val_acc: 0.9992\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 0.0055 - acc: 0.9997 - val_loss: 0.0124 - val_acc: 0.9992\n",
      "6\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 952us/step - loss: 0.0175 - acc: 0.9948 - val_loss: 0.0026 - val_acc: 0.9992acc: 0.9 - ETA: 1s\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 824us/step - loss: 0.0094 - acc: 0.9984 - val_loss: 0.0024 - val_acc: 0.9997\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0012 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 807us/step - loss: 9.2109e-06 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 0.9995\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 812us/step - loss: 2.1697e-06 - acc: 1.0000 - val_loss: 0.0010 - val_acc: 0.9995\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.3540e-06 - acc: 1.0000 - val_loss: 8.9890e-04 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 9.0503e-07 - acc: 1.0000 - val_loss: 9.2249e-04 - val_acc: 0.9997\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 6.1490e-07 - acc: 1.0000 - val_loss: 8.9714e-04 - val_acc: 0.9997: 0s - loss: 6.1907e-07 -\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 4.3732e-07 - acc: 1.0000 - val_loss: 8.2483e-04 - val_acc: 0.9997\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 3.2333e-07 - acc: 1.0000 - val_loss: 7.8185e-04 - val_acc: 0.9997\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 838us/step - loss: 2.4872e-07 - acc: 1.0000 - val_loss: 8.8189e-04 - val_acc: 0.9997\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 802us/step - loss: 2.0161e-07 - acc: 1.0000 - val_loss: 7.9255e-04 - val_acc: 0.9997\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 924us/step - loss: 1.7187e-07 - acc: 1.0000 - val_loss: 8.1351e-04 - val_acc: 0.9997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 795us/step - loss: 1.5290e-07 - acc: 1.0000 - val_loss: 7.6937e-04 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 854us/step - loss: 1.4061e-07 - acc: 1.0000 - val_loss: 7.5644e-04 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 859us/step - loss: 1.3310e-07 - acc: 1.0000 - val_loss: 7.2099e-04 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 870us/step - loss: 1.2813e-07 - acc: 1.0000 - val_loss: 6.8659e-04 - val_acc: 0.9997\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 836us/step - loss: 1.2484e-07 - acc: 1.0000 - val_loss: 7.0504e-04 - val_acc: 0.9997\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 14s 937us/step - loss: 1.2273e-07 - acc: 1.0000 - val_loss: 6.7970e-04 - val_acc: 0.9997\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 14s 952us/step - loss: 1.2140e-07 - acc: 1.0000 - val_loss: 6.6913e-04 - val_acc: 0.9997\n",
      "7\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0287 - acc: 0.9950 - val_loss: 0.0164 - val_acc: 0.9973\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 14s 980us/step - loss: 0.0085 - acc: 0.9988 - val_loss: 0.0044 - val_acc: 0.9995\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.0053 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 853us/step - loss: 0.0055 - acc: 0.9995 - val_loss: 0.0067 - val_acc: 0.9989\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 14s 987us/step - loss: 0.0044 - acc: 0.9996 - val_loss: 0.0089 - val_acc: 0.9995\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 14s 971us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 14s 985us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 14s 983us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 14s 930us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 13s 909us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 801us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 824us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 11s 784us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 806us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 14s 932us/step - loss: 0.0229 - acc: 0.9984 - val_loss: 0.0737 - val_acc: 0.9945\n",
      "8\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0072 - val_acc: 0.9975\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 13s 877us/step - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0086 - val_acc: 0.9981\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0020 - acc: 0.9991 - val_loss: 0.0446 - val_acc: 0.9879\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0130 - val_acc: 0.9975\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 13s 880us/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0044 - val_acc: 0.9981\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 832us/step - loss: 1.3368e-05 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9981\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 798us/step - loss: 1.7601e-06 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9981\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 14s 930us/step - loss: 9.7563e-07 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 0.9981\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 6.0437e-07 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9981\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 835us/step - loss: 4.0342e-07 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9981\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 836us/step - loss: 2.8833e-07 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 0.9981\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 835us/step - loss: 2.2236e-07 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9981\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 836us/step - loss: 1.8249e-07 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 0.9981\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 13s 865us/step - loss: 1.5826e-07 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9981\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 13s 885us/step - loss: 1.4404e-07 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 0.9981\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 868us/step - loss: 1.3516e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9981\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 854us/step - loss: 1.2943e-07 - acc: 1.0000 - val_loss: 0.0101 - val_acc: 0.9978\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 13s 863us/step - loss: 1.2573e-07 - acc: 1.0000 - val_loss: 0.0104 - val_acc: 0.9978\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 14s 963us/step - loss: 1.2332e-07 - acc: 1.0000 - val_loss: 0.0107 - val_acc: 0.9978\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 1.2180e-07 - acc: 1.0000 - val_loss: 0.0110 - val_acc: 0.9978\n",
      "9\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 13s 862us/step - loss: 0.0157 - acc: 0.9946 - val_loss: 0.0126 - val_acc: 0.9981\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 792us/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0049 - val_acc: 0.9995\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 813us/step - loss: 0.0037 - acc: 0.9988 - val_loss: 0.0048 - val_acc: 0.9997\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 819us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0090 - val_acc: 0.9989\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 807us/step - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 13s 895us/step - loss: 1.3769e-05 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 13s 913us/step - loss: 1.1929e-06 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 5.6824e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 859us/step - loss: 4.0103e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 13s 902us/step - loss: 2.9805e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 825us/step - loss: 2.3308e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 13s 909us/step - loss: 1.9296e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 859us/step - loss: 1.6695e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 11s 780us/step - loss: 1.4968e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 797us/step - loss: 1.3902e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 1.3206e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 834us/step - loss: 1.2758e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 11s 787us/step - loss: 1.2475e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 11s 787us/step - loss: 1.2289e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 921us/step - loss: 1.2164e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "10\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 958us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.0230 - val_acc: 0.9931\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 14s 994us/step - loss: 0.0090 - acc: 0.9979 - val_loss: 0.0058 - val_acc: 0.9995\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 811us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 0.0153 - val_acc: 0.9973\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 829us/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0067 - val_acc: 0.9989\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 13s 886us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.0066 - val_acc: 0.9995\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 839us/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.0069 - val_acc: 0.9995\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 806us/step - loss: 0.0067 - acc: 0.9991 - val_loss: 0.0090 - val_acc: 0.9989\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 13s 914us/step - loss: 0.0026 - acc: 0.9998 - val_loss: 0.0089 - val_acc: 0.9995\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 854us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0089 - val_acc: 0.9995\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 13s 894us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0089 - val_acc: 0.9995\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 13s 898us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0089 - val_acc: 0.9995\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 13s 863us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0090 - val_acc: 0.9989\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 14s 944us/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0080 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 13s 860us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0085 - val_acc: 0.9995\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 13s 893us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0086 - val_acc: 0.9995\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 864us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0086 - val_acc: 0.9995\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 796us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0087 - val_acc: 0.9995\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 825us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0087 - val_acc: 0.9995\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 841us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "11\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 15s 1000us/step - loss: 0.0184 - acc: 0.9939 - val_loss: 0.0029 - val_acc: 0.9986\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 806us/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0054 - val_acc: 0.9986\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 805us/step - loss: 0.0018 - acc: 0.9996 - val_loss: 1.0763e-04 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 11s 787us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 6.7894e-04 - val_acc: 0.9997\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 795us/step - loss: 1.8291e-05 - acc: 1.0000 - val_loss: 4.5294e-05 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 791us/step - loss: 1.2914e-06 - acc: 1.0000 - val_loss: 4.2641e-05 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 11s 788us/step - loss: 8.5779e-07 - acc: 1.0000 - val_loss: 3.8495e-05 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 791us/step - loss: 6.0167e-07 - acc: 1.0000 - val_loss: 3.6125e-05 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 792us/step - loss: 4.3144e-07 - acc: 1.0000 - val_loss: 3.0458e-05 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 800us/step - loss: 3.2037e-07 - acc: 1.0000 - val_loss: 2.7714e-05 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 14s 938us/step - loss: 2.4659e-07 - acc: 1.0000 - val_loss: 2.2509e-05 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 921us/step - loss: 1.9958e-07 - acc: 1.0000 - val_loss: 1.7920e-05 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 839us/step - loss: 1.6942e-07 - acc: 1.0000 - val_loss: 1.5012e-05 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 14s 972us/step - loss: 1.5082e-07 - acc: 1.0000 - val_loss: 1.2396e-05 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 897us/step - loss: 1.3893e-07 - acc: 1.0000 - val_loss: 1.0552e-05 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 1.3180e-07 - acc: 1.0000 - val_loss: 8.6064e-06 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.2717e-07 - acc: 1.0000 - val_loss: 7.2855e-06 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 14s 972us/step - loss: 1.2427e-07 - acc: 1.0000 - val_loss: 6.2664e-06 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 1.2242e-07 - acc: 1.0000 - val_loss: 5.1915e-06 - val_acc: 1.0000\n",
      "12\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0161 - acc: 0.9961 - val_loss: 6.1083e-04 - val_acc: 1.0000\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0048 - val_acc: 0.9995TA: 8s - l - ETA: - ETA: 2s - loss: 0.0035 - - ETA: 1s\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 856us/step - loss: 1.4865e-05 - acc: 1.0000 - val_loss: 1.5044e-04 - val_acc: 1.0000\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 1.5956e-06 - acc: 1.0000 - val_loss: 1.8890e-04 - val_acc: 1.00003s - loss: 2.0172e\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14573/14573 [==============================] - 12s 845us/step - loss: 8.1795e-07 - acc: 1.0000 - val_loss: 2.0829e-04 - val_acc: 0.9997\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 833us/step - loss: 5.4306e-07 - acc: 1.0000 - val_loss: 2.2923e-04 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 798us/step - loss: 3.8744e-07 - acc: 1.0000 - val_loss: 2.5188e-04 - val_acc: 0.9997\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 802us/step - loss: 2.9198e-07 - acc: 1.0000 - val_loss: 2.7591e-04 - val_acc: 0.9997\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 800us/step - loss: 2.3368e-07 - acc: 1.0000 - val_loss: 3.0060e-04 - val_acc: 0.9997\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 1.9486e-07 - acc: 1.0000 - val_loss: 3.1969e-04 - val_acc: 0.9997s - loss: 1.9020e - ETA: 1s - loss:\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 800us/step - loss: 1.6870e-07 - acc: 1.0000 - val_loss: 3.4317e-04 - val_acc: 0.9997\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 799us/step - loss: 1.5168e-07 - acc: 1.0000 - val_loss: 3.6786e-04 - val_acc: 0.9997\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 859us/step - loss: 1.4086e-07 - acc: 1.0000 - val_loss: 3.9121e-04 - val_acc: 0.9997\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 819us/step - loss: 1.3357e-07 - acc: 1.0000 - val_loss: 4.1578e-04 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 13s 859us/step - loss: 1.2863e-07 - acc: 1.0000 - val_loss: 4.3761e-04 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 810us/step - loss: 1.2541e-07 - acc: 1.0000 - val_loss: 4.5711e-04 - val_acc: 0.9997acc:\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 840us/step - loss: 1.2320e-07 - acc: 1.0000 - val_loss: 4.7629e-04 - val_acc: 0.9997\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 13s 860us/step - loss: 1.2173e-07 - acc: 1.0000 - val_loss: 4.8636e-04 - val_acc: 0.9997\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 14s 965us/step - loss: 1.2078e-07 - acc: 1.0000 - val_loss: 5.0444e-04 - val_acc: 0.9997\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 922us/step - loss: 1.2017e-07 - acc: 1.0000 - val_loss: 5.1450e-04 - val_acc: 0.999719e-07 - acc: 1.0 - ETA: 0s - loss: 1.2017e-07 - acc:  - ETA: 0s - loss: 1.2018e-07 - acc: 1.0\n",
      "13\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 15s 997us/step - loss: 0.0256 - acc: 0.9920 - val_loss: 0.0147 - val_acc: 0.9959\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 13s 886us/step - loss: 0.0081 - acc: 0.9976 - val_loss: 0.0146 - val_acc: 0.9964\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 0.0059 - acc: 0.9984 - val_loss: 0.0097 - val_acc: 0.9975\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 824us/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0088 - val_acc: 0.9986\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 838us/step - loss: 4.6243e-04 - acc: 0.9997 - val_loss: 0.0265 - val_acc: 0.9951\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 838us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0200 - val_acc: 0.9978\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 856us/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.0091 - val_acc: 0.9989\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 826us/step - loss: 0.0045 - acc: 0.9990 - val_loss: 0.0170 - val_acc: 0.9967\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 14s 938us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0074 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 1.7643e-05 - acc: 1.0000 - val_loss: 0.0106 - val_acc: 0.9989\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 14s 987us/step - loss: 2.5306e-06 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 0.9989\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 13s 920us/step - loss: 5.9611e-07 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 0.9989\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 14s 959us/step - loss: 3.0388e-07 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 0.9989\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 13s 889us/step - loss: 2.3357e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9989\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 833us/step - loss: 1.9467e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9989\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 822us/step - loss: 1.6978e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9989\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 878us/step - loss: 1.5235e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9989\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 14s 928us/step - loss: 1.4160e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9989\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 838us/step - loss: 1.3373e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9989\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 904us/step - loss: 1.2880e-07 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 0.9989\n",
      "14\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 15s 998us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0087 - val_acc: 0.9984\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 13s 874us/step - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0172 - val_acc: 0.9959\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 13s 893us/step - loss: 0.0048 - acc: 0.9986 - val_loss: 0.0143 - val_acc: 0.9964\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 13s 877us/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0053 - val_acc: 0.9984\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 812us/step - loss: 1.1685e-04 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9995\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 14s 967us/step - loss: 5.4444e-05 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 0.9989\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 14s 990us/step - loss: 1.1504e-06 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9989\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 13s 869us/step - loss: 6.9614e-07 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9989\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 818us/step - loss: 4.7876e-07 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 0.9989\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 830us/step - loss: 3.5383e-07 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 0.9989\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 852us/step - loss: 2.7503e-07 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 0.9989\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 2.2133e-07 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9989\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 14s 939us/step - loss: 1.8654e-07 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9989\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 854us/step - loss: 1.6326e-07 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9989\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 14s 962us/step - loss: 1.4811e-07 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 0.9989\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 14s 962us/step - loss: 1.3739e-07 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 0.9989\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 876us/step - loss: 1.3064e-07 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9989\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 1.2649e-07 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 0.9989\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 1.2388e-07 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 0.9989\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 902us/step - loss: 1.2218e-07 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 0.9989\n",
      "15\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0238 - acc: 0.9909 - val_loss: 0.0119 - val_acc: 0.9956\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 14s 938us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.0067 - val_acc: 0.9978\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 14s 931us/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.0357 - val_acc: 0.9931\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 14s 931us/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0167 - val_acc: 0.9964\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 841us/step - loss: 0.0046 - acc: 0.9987 - val_loss: 0.0065 - val_acc: 0.9984\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 807us/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0066 - val_acc: 0.9978\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 811us/step - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0062 - val_acc: 0.9967\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 818us/step - loss: 2.9429e-04 - acc: 0.9999 - val_loss: 0.0048 - val_acc: 0.9989\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 797us/step - loss: 3.0979e-06 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9986\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 807us/step - loss: 1.4617e-06 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9986\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 804us/step - loss: 8.4011e-07 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 0.9986\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 794us/step - loss: 5.3724e-07 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9986\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 796us/step - loss: 3.6343e-07 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 0.9986\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 796us/step - loss: 2.6580e-07 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 0.9986\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 804us/step - loss: 2.0910e-07 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 0.9986\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 815us/step - loss: 1.7532e-07 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 0.9986\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 1.5528e-07 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 0.9986\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 840us/step - loss: 1.4248e-07 - acc: 1.0000 - val_loss: 0.0072 - val_acc: 0.9986\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 852us/step - loss: 1.3434e-07 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9986\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 862us/step - loss: 1.2910e-07 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 0.9989\n",
      "16\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 942us/step - loss: 0.0230 - acc: 0.9928 - val_loss: 0.0081 - val_acc: 0.9978\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 805us/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0110 - val_acc: 0.9970\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 811us/step - loss: 0.0041 - acc: 0.9989 - val_loss: 3.6157e-04 - val_acc: 0.9997\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 800us/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.0091 - val_acc: 0.9975\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 834us/step - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0045 - val_acc: 0.9981\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 831us/step - loss: 3.6879e-04 - acc: 0.9999 - val_loss: 5.8583e-04 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 818us/step - loss: 2.8654e-06 - acc: 1.0000 - val_loss: 5.5968e-04 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 826us/step - loss: 1.5101e-06 - acc: 1.0000 - val_loss: 5.3800e-04 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 8.9836e-07 - acc: 1.0000 - val_loss: 5.2321e-04 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 5.7811e-07 - acc: 1.0000 - val_loss: 5.0299e-04 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 806us/step - loss: 3.9934e-07 - acc: 1.0000 - val_loss: 4.8800e-04 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 802us/step - loss: 2.9144e-07 - acc: 1.0000 - val_loss: 4.6807e-04 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 801us/step - loss: 2.2609e-07 - acc: 1.0000 - val_loss: 4.5351e-04 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 805us/step - loss: 1.8727e-07 - acc: 1.0000 - val_loss: 4.3963e-04 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 805us/step - loss: 1.6289e-07 - acc: 1.0000 - val_loss: 4.2851e-04 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 802us/step - loss: 1.4735e-07 - acc: 1.0000 - val_loss: 4.1577e-04 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 808us/step - loss: 1.3722e-07 - acc: 1.0000 - val_loss: 4.0614e-04 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 791us/step - loss: 1.3070e-07 - acc: 1.0000 - val_loss: 3.9058e-04 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 808us/step - loss: 1.2653e-07 - acc: 1.0000 - val_loss: 3.8179e-04 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 801us/step - loss: 1.2395e-07 - acc: 1.0000 - val_loss: 3.7254e-04 - val_acc: 1.0000\n",
      "17\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 13s 916us/step - loss: 0.0185 - acc: 0.9960 - val_loss: 0.0067 - val_acc: 0.9986\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 802us/step - loss: 0.0080 - acc: 0.9986 - val_loss: 7.0383e-04 - val_acc: 0.9997\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 800us/step - loss: 0.0051 - acc: 0.9989 - val_loss: 6.5200e-04 - val_acc: 0.9997\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 807us/step - loss: 0.0044 - acc: 0.9995 - val_loss: 0.0022 - val_acc: 0.9995\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 848us/step - loss: 0.0032 - acc: 0.9995 - val_loss: 0.0016 - val_acc: 0.9989\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 1.4204e-04 - acc: 0.9999 - val_loss: 4.9188e-04 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 813us/step - loss: 2.2976e-06 - acc: 1.0000 - val_loss: 2.9747e-04 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 814us/step - loss: 5.7453e-07 - acc: 1.0000 - val_loss: 2.5559e-04 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 824us/step - loss: 3.8415e-07 - acc: 1.0000 - val_loss: 2.1949e-04 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 810us/step - loss: 2.8702e-07 - acc: 1.0000 - val_loss: 1.8564e-04 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 808us/step - loss: 2.2711e-07 - acc: 1.0000 - val_loss: 1.5892e-04 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 805us/step - loss: 1.8990e-07 - acc: 1.0000 - val_loss: 1.3881e-04 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 798us/step - loss: 1.6725e-07 - acc: 1.0000 - val_loss: 1.2140e-04 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 810us/step - loss: 1.5134e-07 - acc: 1.0000 - val_loss: 1.0734e-04 - val_acc: 1.0000\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14573/14573 [==============================] - 12s 816us/step - loss: 1.4074e-07 - acc: 1.0000 - val_loss: 9.2545e-05 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 810us/step - loss: 1.3346e-07 - acc: 1.0000 - val_loss: 8.8409e-05 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 807us/step - loss: 1.2880e-07 - acc: 1.0000 - val_loss: 7.7016e-05 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 801us/step - loss: 1.2564e-07 - acc: 1.0000 - val_loss: 6.8180e-05 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 812us/step - loss: 1.2354e-07 - acc: 1.0000 - val_loss: 6.0643e-05 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 805us/step - loss: 1.2217e-07 - acc: 1.0000 - val_loss: 5.3668e-05 - val_acc: 1.0000\n",
      "18\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 929us/step - loss: 0.0245 - acc: 0.9933 - val_loss: 0.0124 - val_acc: 0.9970\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 0.0090 - acc: 0.9980 - val_loss: 0.0173 - val_acc: 0.9953\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0053 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 840us/step - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0340 - val_acc: 0.9970\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 0.0087 - acc: 0.9983 - val_loss: 0.0066 - val_acc: 0.9989\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 831us/step - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0110 - val_acc: 0.9986\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0099 - val_acc: 0.9992\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 841us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0099 - val_acc: 0.9989\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 13s 863us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 806us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0097 - val_acc: 0.9992\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 801us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0097 - val_acc: 0.9992\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 802us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0096 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 824us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0096 - val_acc: 0.9992\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 820us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0096 - val_acc: 0.9992\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 820us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0095 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 818us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0095 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 824us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0095 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 815us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0095 - val_acc: 0.9992\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 809us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0094 - val_acc: 0.9992\n",
      "19\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 932us/step - loss: 0.0185 - acc: 0.9960 - val_loss: 0.0049 - val_acc: 0.9989\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 813us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.0110 - val_acc: 0.9978\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 0.0104 - acc: 0.9986 - val_loss: 0.0139 - val_acc: 0.9978\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 829us/step - loss: 0.0111 - acc: 0.9988 - val_loss: 0.0144 - val_acc: 0.9984\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 827us/step - loss: 0.0045 - acc: 0.9995 - val_loss: 0.0076 - val_acc: 0.9992\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 834us/step - loss: 0.0032 - acc: 0.9995 - val_loss: 0.0215 - val_acc: 0.9981\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 812us/step - loss: 0.0028 - acc: 0.9997 - val_loss: 0.0225 - val_acc: 0.9981\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 803us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 808us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 806us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 807us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 822us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 819us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 13s 885us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 863us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 812us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 812us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 808us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 809us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "20\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 968us/step - loss: 0.0331 - acc: 0.9898 - val_loss: 0.0114 - val_acc: 0.9956\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 812us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0122 - val_acc: 0.9945\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0047 - val_acc: 0.9984\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 827us/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.0191 - val_acc: 0.9942\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 0.0070 - acc: 0.9979 - val_loss: 0.0073 - val_acc: 0.9975\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 811us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0036 - val_acc: 0.9992\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 6.5082e-06 - acc: 1.0000 - val_loss: 0.0041 - val_acc: 0.9995\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 819us/step - loss: 1.5364e-06 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9995\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 7.9955e-07 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9995\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 4.9237e-07 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9995\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 829us/step - loss: 3.4912e-07 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9995\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 829us/step - loss: 2.5879e-07 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 0.9995\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 2.0187e-07 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 1.6420e-07 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9995\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 834us/step - loss: 1.4714e-07 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 0.9995\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 829us/step - loss: 1.3696e-07 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9995\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 813us/step - loss: 1.3078e-07 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 0.9995\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 810us/step - loss: 1.2718e-07 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9995\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 814us/step - loss: 1.2425e-07 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9995\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 831us/step - loss: 1.2264e-07 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9995\n",
      "21\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 967us/step - loss: 0.0129 - acc: 0.9961 - val_loss: 0.0026 - val_acc: 0.9992\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 815us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0035 - val_acc: 0.9992\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 819us/step - loss: 0.0018 - acc: 0.9997 - val_loss: 3.7319e-04 - val_acc: 0.9997\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 818us/step - loss: 9.2234e-06 - acc: 1.0000 - val_loss: 3.0263e-04 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 1.7615e-06 - acc: 1.0000 - val_loss: 2.7811e-04 - val_acc: 1.0000\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 13s 862us/step - loss: 1.0635e-06 - acc: 1.0000 - val_loss: 2.6158e-04 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 13s 915us/step - loss: 6.9409e-07 - acc: 1.0000 - val_loss: 2.4793e-04 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 13s 877us/step - loss: 4.6866e-07 - acc: 1.0000 - val_loss: 2.4055e-04 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 862us/step - loss: 3.2814e-07 - acc: 1.0000 - val_loss: 2.3516e-04 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 813us/step - loss: 2.4207e-07 - acc: 1.0000 - val_loss: 2.2801e-04 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 822us/step - loss: 1.9151e-07 - acc: 1.0000 - val_loss: 2.2149e-04 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 814us/step - loss: 1.6229e-07 - acc: 1.0000 - val_loss: 2.1554e-04 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 815us/step - loss: 1.4566e-07 - acc: 1.0000 - val_loss: 2.1098e-04 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 818us/step - loss: 1.3557e-07 - acc: 1.0000 - val_loss: 2.0725e-04 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 807us/step - loss: 1.2940e-07 - acc: 1.0000 - val_loss: 2.0222e-04 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 814us/step - loss: 1.2554e-07 - acc: 1.0000 - val_loss: 1.9761e-04 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 813us/step - loss: 1.2320e-07 - acc: 1.0000 - val_loss: 1.9289e-04 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 808us/step - loss: 1.2170e-07 - acc: 1.0000 - val_loss: 1.8468e-04 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 820us/step - loss: 1.2080e-07 - acc: 1.0000 - val_loss: 1.8004e-04 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 1.2015e-07 - acc: 1.0000 - val_loss: 1.7516e-04 - val_acc: 1.0000\n",
      "22\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 980us/step - loss: 0.0218 - acc: 0.9929 - val_loss: 0.0166 - val_acc: 0.9959\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 852us/step - loss: 0.0064 - acc: 0.9986 - val_loss: 0.0153 - val_acc: 0.9973\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 13s 872us/step - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0143 - val_acc: 0.9975\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 13s 888us/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0099 - val_acc: 0.9989\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 848us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0097 - val_acc: 0.9989\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0099 - val_acc: 0.9989\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 820us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0100 - val_acc: 0.9989\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 818us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0100 - val_acc: 0.9989\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 820us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0100 - val_acc: 0.9989\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 826us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0100 - val_acc: 0.9989\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 821us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0101 - val_acc: 0.9989\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 817us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0100 - val_acc: 0.9989\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0102 - val_acc: 0.9989\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0101 - val_acc: 0.9989\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 836us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0102 - val_acc: 0.9989\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 883us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0102 - val_acc: 0.9989\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 860us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0102 - val_acc: 0.9989\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 13s 867us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0103 - val_acc: 0.9989\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 836us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0103 - val_acc: 0.9989\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0103 - val_acc: 0.9989\n",
      "23\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 970us/step - loss: 0.0181 - acc: 0.9942 - val_loss: 0.0047 - val_acc: 0.9989\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 856us/step - loss: 0.0063 - acc: 0.9984 - val_loss: 0.0041 - val_acc: 0.9997\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0075 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 13s 873us/step - loss: 0.0080 - acc: 0.9986 - val_loss: 0.0049 - val_acc: 0.9989\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 13s 874us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0023 - val_acc: 0.9992\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 13s 874us/step - loss: 1.3622e-04 - acc: 0.9999 - val_loss: 3.2409e-04 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 13s 865us/step - loss: 3.5929e-06 - acc: 1.0000 - val_loss: 2.5461e-04 - val_acc: 1.0000\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14573/14573 [==============================] - 13s 906us/step - loss: 4.5107e-07 - acc: 1.0000 - val_loss: 2.6045e-04 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 874us/step - loss: 3.3158e-07 - acc: 1.0000 - val_loss: 3.0935e-04 - val_acc: 0.9997\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 831us/step - loss: 2.5943e-07 - acc: 1.0000 - val_loss: 3.6601e-04 - val_acc: 0.9997\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 822us/step - loss: 2.1240e-07 - acc: 1.0000 - val_loss: 4.3819e-04 - val_acc: 0.9997\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 819us/step - loss: 1.8251e-07 - acc: 1.0000 - val_loss: 4.9770e-04 - val_acc: 0.9997\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 825us/step - loss: 1.6187e-07 - acc: 1.0000 - val_loss: 5.3905e-04 - val_acc: 0.9997\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 816us/step - loss: 1.4846e-07 - acc: 1.0000 - val_loss: 5.7600e-04 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 818us/step - loss: 1.3895e-07 - acc: 1.0000 - val_loss: 6.0571e-04 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 820us/step - loss: 1.3258e-07 - acc: 1.0000 - val_loss: 6.4315e-04 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 831us/step - loss: 1.2812e-07 - acc: 1.0000 - val_loss: 7.0503e-04 - val_acc: 0.9995\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 828us/step - loss: 1.2521e-07 - acc: 1.0000 - val_loss: 7.3287e-04 - val_acc: 0.9995\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 821us/step - loss: 1.2333e-07 - acc: 1.0000 - val_loss: 7.8119e-04 - val_acc: 0.9995\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 836us/step - loss: 1.2194e-07 - acc: 1.0000 - val_loss: 8.3563e-04 - val_acc: 0.9995\n",
      "24\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 968us/step - loss: 0.0278 - acc: 0.9906 - val_loss: 0.0248 - val_acc: 0.9934\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 0.0095 - acc: 0.9973 - val_loss: 0.0507 - val_acc: 0.9882\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 820us/step - loss: 0.0085 - acc: 0.9977 - val_loss: 0.0054 - val_acc: 0.9978\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 828us/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.0059 - val_acc: 0.9981\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 828us/step - loss: 0.0064 - acc: 0.9984 - val_loss: 0.0059 - val_acc: 0.9984\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 830us/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.0072 - val_acc: 0.9973\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 829us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.0071 - val_acc: 0.9992\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 837us/step - loss: 0.0034 - acc: 0.9998 - val_loss: 0.0088 - val_acc: 0.9981\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 831us/step - loss: 0.0164 - acc: 0.9981 - val_loss: 0.0142 - val_acc: 0.9989\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 0.0111 - acc: 0.9988 - val_loss: 0.0168 - val_acc: 0.9975\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0048 - val_acc: 0.9986\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.0020 - val_acc: 0.9995\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 899us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0021 - val_acc: 0.9995\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 13s 860us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 835us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0026 - val_acc: 0.9995\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 835us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0027 - val_acc: 0.9995\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0027 - val_acc: 0.9995\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 829us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 838us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0029 - val_acc: 0.9995\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0031 - val_acc: 0.9995\n",
      "25\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0114 - val_acc: 0.9967\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 848us/step - loss: 0.0073 - acc: 0.9979 - val_loss: 0.0137 - val_acc: 0.9964\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 834us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0101 - val_acc: 0.9975\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 0.0022 - acc: 0.9997 - val_loss: 0.0082 - val_acc: 0.9970\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 823us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0054 - val_acc: 0.9992\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 2.3800e-05 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9989\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 832us/step - loss: 1.0375e-06 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9989\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 835us/step - loss: 5.5058e-07 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9992\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 837us/step - loss: 4.1633e-07 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 833us/step - loss: 3.2535e-07 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 2.6016e-07 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 0.9992\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 835us/step - loss: 2.1378e-07 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9992\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 836us/step - loss: 1.8225e-07 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 831us/step - loss: 1.6058e-07 - acc: 1.0000 - val_loss: 0.0079 - val_acc: 0.9992\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 1.4589e-07 - acc: 1.0000 - val_loss: 0.0080 - val_acc: 0.9992\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 1.3619e-07 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 892us/step - loss: 1.2988e-07 - acc: 1.0000 - val_loss: 0.0082 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 1.2595e-07 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 1.2346e-07 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 0.9992\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 829us/step - loss: 1.2185e-07 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 0.9992\n",
      "26\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 14s 967us/step - loss: 0.0350 - acc: 0.9946 - val_loss: 0.0443 - val_acc: 0.9901\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 0.0139 - acc: 0.9986 - val_loss: 0.0421 - val_acc: 0.9956\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 0.0095 - acc: 0.9988 - val_loss: 0.0255 - val_acc: 0.9978\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 0.0079 - acc: 0.9990 - val_loss: 0.0058 - val_acc: 0.9992\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0041 - acc: 0.9996 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 13s 886us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 843us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 854us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 13s 868us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 872us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 13s 874us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 857us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 839us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 839us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.99973s - loss: 0.0030 -\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 841us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 859us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "27\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0246 - acc: 0.9927 - val_loss: 0.0080 - val_acc: 0.9967\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 13s 897us/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0056 - val_acc: 0.9986\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 0.0033 - acc: 0.9997 - val_loss: 0.0050 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0050 - val_acc: 0.9995\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 12s 854us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0051 - val_acc: 0.9995\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 848us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0051 - val_acc: 0.9995\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 856us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 854us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 855us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 853us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 12s 856us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 861us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 848us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 857us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 851us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 845us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.0052 - val_acc: 0.9995\n",
      "28\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0044 - val_acc: 0.9992\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 846us/step - loss: 0.0106 - acc: 0.9980 - val_loss: 0.0112 - val_acc: 0.9978\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 12s 853us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 0.0047 - val_acc: 0.9989\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 13s 867us/step - loss: 0.0052 - acc: 0.9993 - val_loss: 0.0029 - val_acc: 0.9992\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 13s 882us/step - loss: 0.0140 - acc: 0.9987 - val_loss: 0.0283 - val_acc: 0.9970\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 13s 866us/step - loss: 0.0223 - acc: 0.9978 - val_loss: 0.0292 - val_acc: 0.9975\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 12s 856us/step - loss: 0.0089 - acc: 0.9991 - val_loss: 0.0052 - val_acc: 0.9992\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 13s 863us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 0.0049 - val_acc: 0.9995\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0048 - val_acc: 0.9995\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 12s 847us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0048 - val_acc: 0.9995\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 13s 859us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0047 - val_acc: 0.9995\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 12s 857us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0047 - val_acc: 0.9997\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 864us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0047 - val_acc: 0.9997\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 13s 861us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0046 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 12s 842us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0046 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 12s 844us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 12s 855us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 12s 849us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 12s 852us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 12s 850us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "29\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0216 - acc: 0.9928 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 12s 857us/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.0323 - val_acc: 0.9929\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 13s 859us/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.0011 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 13s 877us/step - loss: 0.0072 - acc: 0.9987 - val_loss: 0.0032 - val_acc: 0.9989\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 13s 862us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 0.0015 - val_acc: 0.9989\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 12s 850us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0015 - val_acc: 0.9995\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 13s 858us/step - loss: 0.0190 - acc: 0.9977 - val_loss: 0.0194 - val_acc: 0.9953\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 13s 885us/step - loss: 0.0055 - acc: 0.9995 - val_loss: 0.0070 - val_acc: 0.9992\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 893us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0042 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 13s 860us/step - loss: 0.0049 - acc: 0.9992 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 12s 855us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 13s 902us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 902us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 13s 891us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 13s 897us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 858us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 900us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0017 - val_acc: 0.9995\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 14s 965us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 14s 992us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0016 - val_acc: 0.9995\n",
      "30\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0224 - val_acc: 0.9940\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0124 - val_acc: 0.9975\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 3.3435e-04 - acc: 0.9999 - val_loss: 0.0137 - val_acc: 0.9986\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.5350e-04 - acc: 0.9999 - val_loss: 0.0126 - val_acc: 0.9984\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 14s 933us/step - loss: 2.6333e-06 - acc: 1.0000 - val_loss: 0.0129 - val_acc: 0.9984\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.3880e-06 - acc: 1.0000 - val_loss: 0.0132 - val_acc: 0.9984\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 8.9354e-07 - acc: 1.0000 - val_loss: 0.0134 - val_acc: 0.9984\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 6.0759e-07 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 0.9984\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 4.3552e-07 - acc: 1.0000 - val_loss: 0.0139 - val_acc: 0.9984\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 3.2368e-07 - acc: 1.0000 - val_loss: 0.0141 - val_acc: 0.9984\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 2.5330e-07 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 0.9984\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 2.0632e-07 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9984\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 1.7580e-07 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9984\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.5602e-07 - acc: 1.0000 - val_loss: 0.0151 - val_acc: 0.9984\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 14s 968us/step - loss: 1.4300e-07 - acc: 1.0000 - val_loss: 0.0153 - val_acc: 0.9984\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 923us/step - loss: 1.3459e-07 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9984\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 1.2898e-07 - acc: 1.0000 - val_loss: 0.0156 - val_acc: 0.9984\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 14s 984us/step - loss: 1.2551e-07 - acc: 1.0000 - val_loss: 0.0157 - val_acc: 0.9984\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.2323e-07 - acc: 1.0000 - val_loss: 0.0159 - val_acc: 0.9984\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 923us/step - loss: 1.2176e-07 - acc: 1.0000 - val_loss: 0.0160 - val_acc: 0.9984\n",
      "31\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0249 - acc: 0.9919 - val_loss: 0.0021 - val_acc: 0.9989\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 13s 888us/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0096 - val_acc: 0.9981\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 13s 875us/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.0018 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 14s 976us/step - loss: 0.0012 - acc: 0.9999 - val_loss: 0.0022 - val_acc: 0.9995\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9995\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0023 - val_acc: 0.9995\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0024 - val_acc: 0.9995\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 14s 962us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0025 - val_acc: 0.9995\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 920us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0026 - val_acc: 0.9995\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 13s 911us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0026 - val_acc: 0.9995\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0027 - val_acc: 0.9995\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0028 - val_acc: 0.9995\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0029 - val_acc: 0.9995\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0030 - val_acc: 0.9995\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0031 - val_acc: 0.9995\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 15s 999us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0032 - val_acc: 0.9995\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 14s 970us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0033 - val_acc: 0.9995 - loss:\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0034 - val_acc: 0.9995\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 14s 929us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0035 - val_acc: 0.9995\n",
      "32\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0426 - acc: 0.9935 - val_loss: 0.2749 - val_acc: 0.9819\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 14s 975us/step - loss: 0.0322 - acc: 0.9972 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 0.0069 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0131 - acc: 0.9991 - val_loss: 0.0415 - val_acc: 0.9973\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0250 - acc: 0.9979 - val_loss: 0.0321 - val_acc: 0.9973\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0137 - acc: 0.9990 - val_loss: 0.0126 - val_acc: 0.9992\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0096 - acc: 0.9992 - val_loss: 0.0088 - val_acc: 0.9995\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 13s 890us/step - loss: 0.0119 - acc: 0.9992 - val_loss: 0.0291 - val_acc: 0.9978\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 13s 876us/step - loss: 0.0163 - acc: 0.9988 - val_loss: 0.0134 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 13s 880us/step - loss: 0.0036 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 13s 876us/step - loss: 0.0206 - acc: 0.9984 - val_loss: 0.0177 - val_acc: 0.9989\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 13s 871us/step - loss: 0.0054 - acc: 0.9997 - val_loss: 0.0177 - val_acc: 0.9989\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 13s 907us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 13s 875us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 13s 875us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 884us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 13s 872us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 13s 878us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 13s 879us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 13s 886us/step - loss: 0.0044 - acc: 0.9997 - val_loss: 0.0133 - val_acc: 0.9992\n",
      "33\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0184 - acc: 0.9955 - val_loss: 0.0051 - val_acc: 0.9986\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.0059 - val_acc: 0.9975\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0054 - acc: 0.9988 - val_loss: 7.5798e-04 - val_acc: 0.9997\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 14s 962us/step - loss: 2.2867e-05 - acc: 1.0000 - val_loss: 3.7083e-04 - val_acc: 0.9997\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 13s 922us/step - loss: 1.3629e-06 - acc: 1.0000 - val_loss: 2.9109e-04 - val_acc: 0.9997\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 7.2688e-07 - acc: 1.0000 - val_loss: 2.5280e-04 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 4.7176e-07 - acc: 1.0000 - val_loss: 2.1642e-04 - val_acc: 0.9997\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 3.3922e-07 - acc: 1.0000 - val_loss: 1.9854e-04 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 2.5733e-07 - acc: 1.0000 - val_loss: 1.7999e-04 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 2.0825e-07 - acc: 1.0000 - val_loss: 1.6555e-04 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.7645e-07 - acc: 1.0000 - val_loss: 1.4857e-04 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 1.5674e-07 - acc: 1.0000 - val_loss: 1.3845e-04 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.4343e-07 - acc: 1.0000 - val_loss: 1.2843e-04 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 1.3504e-07 - acc: 1.0000 - val_loss: 1.1971e-04 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 14s 966us/step - loss: 1.2952e-07 - acc: 1.0000 - val_loss: 1.1006e-04 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.2587e-07 - acc: 1.0000 - val_loss: 1.0206e-04 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.2352e-07 - acc: 1.0000 - val_loss: 9.4058e-05 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.2194e-07 - acc: 1.0000 - val_loss: 8.8020e-05 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 14s 980us/step - loss: 1.2092e-07 - acc: 1.0000 - val_loss: 8.1717e-05 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 14s 961us/step - loss: 1.2027e-07 - acc: 1.0000 - val_loss: 7.5777e-05 - val_acc: 1.0000\n",
      "34\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0234 - acc: 0.9918 - val_loss: 0.0164 - val_acc: 0.9929\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 14s 967us/step - loss: 0.0046 - acc: 0.9988 - val_loss: 8.1648e-04 - val_acc: 0.9997\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 14s 963us/step - loss: 0.0063 - acc: 0.9986 - val_loss: 0.0110 - val_acc: 0.9975\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 14s 965us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0047 - val_acc: 0.9989\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 14s 987us/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0078 - val_acc: 0.9984\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 8.6090e-05 - acc: 1.0000 - val_loss: 0.0047 - val_acc: 0.9992\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 2.4555e-06 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9992\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.0702e-06 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 0.9992\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 6.4805e-07 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 14s 964us/step - loss: 4.3869e-07 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 14s 968us/step - loss: 3.1807e-07 - acc: 1.0000 - val_loss: 0.0059 - val_acc: 0.9992\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 14s 947us/step - loss: 2.4352e-07 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 0.9992\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.9729e-07 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 1.6944e-07 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 0.9992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.5007e-07 - acc: 1.0000 - val_loss: 0.0067 - val_acc: 0.99921.\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 1.3855e-07 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.3124e-07 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 14s 928us/step - loss: 1.2652e-07 - acc: 1.0000 - val_loss: 0.0071 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.2377e-07 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9992\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 14s 955us/step - loss: 1.2208e-07 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9992\n",
      "35\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0155 - acc: 0.9951 - val_loss: 0.0012 - val_acc: 0.9995\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 14s 947us/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.0056 - val_acc: 0.9986\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 14s 984us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0021 - val_acc: 0.9997\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 5.6038e-04 - acc: 0.9997 - val_loss: 1.7706e-04 - val_acc: 1.0000\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 5.1852e-06 - acc: 1.0000 - val_loss: 2.9131e-04 - val_acc: 0.9997\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.1913e-06 - acc: 1.0000 - val_loss: 2.6376e-04 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 21s 1ms/step - loss: 7.3680e-07 - acc: 1.0000 - val_loss: 2.4608e-04 - val_acc: 0.99974163e-07 - acc: 1.0\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 5.1065e-07 - acc: 1.0000 - val_loss: 2.2980e-04 - val_acc: 0.9997\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 3.6893e-07 - acc: 1.0000 - val_loss: 2.0714e-04 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 2.8295e-07 - acc: 1.0000 - val_loss: 1.8592e-04 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 2.2798e-07 - acc: 1.0000 - val_loss: 1.6876e-04 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 22s 1ms/step - loss: 1.9129e-07 - acc: 1.0000 - val_loss: 1.5381e-04 - val_acc: 1.0000: 0s - loss: 1.9387e-0\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.6746e-07 - acc: 1.0000 - val_loss: 1.3674e-04 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 1.5078e-07 - acc: 1.0000 - val_loss: 1.2595e-04 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 14s 993us/step - loss: 1.3991e-07 - acc: 1.0000 - val_loss: 1.1424e-04 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 13s 924us/step - loss: 1.3269e-07 - acc: 1.0000 - val_loss: 1.0518e-04 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 14s 955us/step - loss: 1.2806e-07 - acc: 1.0000 - val_loss: 9.6003e-05 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 1.2485e-07 - acc: 1.0000 - val_loss: 8.7442e-05 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 15s 998us/step - loss: 1.2280e-07 - acc: 1.0000 - val_loss: 8.1261e-05 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.2145e-07 - acc: 1.0000 - val_loss: 7.5231e-05 - val_acc: 1.0000\n",
      "36\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0347 - acc: 0.9942 - val_loss: 0.0189 - val_acc: 0.9959\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0064 - acc: 0.9988 - val_loss: 0.0135 - val_acc: 0.9984\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 23s 2ms/step - loss: 0.0053 - acc: 0.9992 - val_loss: 0.0080 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0010 - acc: 0.9999 - val_loss: 0.0120 - val_acc: 0.9986\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 8.2977e-06 - acc: 1.0000 - val_loss: 0.0101 - val_acc: 0.9992\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 3.2721e-07 - acc: 1.0000 - val_loss: 0.0101 - val_acc: 0.9992\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 14s 927us/step - loss: 2.6688e-07 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 0.9992\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 2.2532e-07 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 0.9992\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 14s 992us/step - loss: 1.9526e-07 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 1.7338e-07 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 14s 987us/step - loss: 1.5807e-07 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 0.9992\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 14s 978us/step - loss: 1.4706e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.3950e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.3413e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 1.3011e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 1.2711e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.2487e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 1.2321e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 1.2200e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.2107e-07 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9992\n",
      "37\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 21s 1ms/step - loss: 0.0233 - acc: 0.9954 - val_loss: 0.0069 - val_acc: 0.9989\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0261 - val_acc: 0.9956\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0054 - acc: 0.9990 - val_loss: 0.0055 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0035 - acc: 0.9992 - val_loss: 0.0071 - val_acc: 0.9986\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 3.0447e-04 - acc: 0.9999 - val_loss: 0.0045 - val_acc: 0.9997\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 1.3621e-06 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 8.2777e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 5.8456e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 4.3094e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 3.2957e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 2.6064e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 2.1291e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997TA: 2s - loss: 1.9621e-07 - acc: 1.000 - \n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.8113e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 1.6072e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.4683e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 1.3707e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 1.3077e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.2671e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997-07 - a\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 1.2403e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997loss: 1.24\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.2228e-07 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9997\n",
      "38\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 23s 2ms/step - loss: 0.0134 - acc: 0.9966 - val_loss: 0.0013 - val_acc: 0.9992\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 0.0059 - acc: 0.9992 - val_loss: 0.0132 - val_acc: 0.9986\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0046 - acc: 0.9997 - val_loss: 9.4018e-04 - val_acc: 0.9997\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0062 - acc: 0.9993 - val_loss: 0.0047 - val_acc: 0.9995\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0074 - acc: 0.9992 - val_loss: 0.0089 - val_acc: 0.9986\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0069 - acc: 0.9987 - val_loss: 9.5154e-05 - val_acc: 1.0000\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0016 - acc: 0.9998 - val_loss: 7.5590e-05 - val_acc: 1.0000\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 6.3125e-06 - val_acc: 1.0000\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 3.7470e-06 - val_acc: 1.0000\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 2.4806e-06 - val_acc: 1.0000\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 1.7723e-06 - val_acc: 1.0000\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 1.3367e-06 - val_acc: 1.0000\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 1.0402e-06 - val_acc: 1.0000\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 8.3920e-07 - val_acc: 1.0000\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 6.7507e-07 - val_acc: 1.0000\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 5.7698e-07 - val_acc: 1.0000\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 5.0035e-07 - val_acc: 1.0000\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 22s 2ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 4.4756e-07 - val_acc: 1.0000\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 4.0382e-07 - val_acc: 1.0000\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 3.6734e-07 - val_acc: 1.0000\n",
      "39\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 23s 2ms/step - loss: 0.0280 - acc: 0.9951 - val_loss: 0.0246 - val_acc: 0.9973\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0100 - acc: 0.9988 - val_loss: 0.0115 - val_acc: 0.9986\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 0.0113 - acc: 0.9989 - val_loss: 0.0061 - val_acc: 0.9995\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0146 - acc: 0.9989 - val_loss: 0.0182 - val_acc: 0.9984\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 22s 1ms/step - loss: 0.0085 - acc: 0.9992 - val_loss: 0.0162 - val_acc: 0.9981\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 0.0069 - acc: 0.9995 - val_loss: 0.0109 - val_acc: 0.9984\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0122 - acc: 0.9991 - val_loss: 0.0138 - val_acc: 0.9989\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0080 - acc: 0.9994 - val_loss: 0.0142 - val_acc: 0.9986\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0078 - acc: 0.9995 - val_loss: 0.0135 - val_acc: 0.9992\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0135 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 21s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0135 - val_acc: 0.9992\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0135 - val_acc: 0.9992\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0135 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0134 - val_acc: 0.9992\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0134 - val_acc: 0.9992\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0134 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0134 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0134 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0134 - val_acc: 0.9992\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0077 - acc: 0.9995 - val_loss: 0.0134 - val_acc: 0.9992\n",
      "40\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 0.0258 - acc: 0.9939 - val_loss: 0.0134 - val_acc: 0.9978\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 0.0092 - acc: 0.9986 - val_loss: 0.0115 - val_acc: 0.9984\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0043 - acc: 0.9996 - val_loss: 0.0079 - val_acc: 0.9986\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0028 - acc: 0.9998 - val_loss: 0.1086 - val_acc: 0.9896\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0206 - acc: 0.9979 - val_loss: 0.0173 - val_acc: 0.9978\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 19s 1ms/step - loss: 0.0045 - acc: 0.9994 - val_loss: 0.0095 - val_acc: 0.9989\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 23s 2ms/step - loss: 0.0047 - acc: 0.9994 - val_loss: 0.0101 - val_acc: 0.9989\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14573/14573 [==============================] - 22s 2ms/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0080 - val_acc: 0.9995\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 21s 1ms/step - loss: 1.5632e-04 - acc: 0.9999 - val_loss: 0.0082 - val_acc: 0.9995\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 1.9153e-06 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 0.9992\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 20s 1ms/step - loss: 3.1695e-07 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 0.9992\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 17s 1ms/step - loss: 2.4325e-07 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 0.9992\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 2.0542e-07 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 0.9992\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.8134e-07 - acc: 1.0000 - val_loss: 0.0087 - val_acc: 0.9992\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.6494e-07 - acc: 1.0000 - val_loss: 0.0088 - val_acc: 0.9992\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.5216e-07 - acc: 1.0000 - val_loss: 0.0088 - val_acc: 0.9992\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.4301e-07 - acc: 1.0000 - val_loss: 0.0089 - val_acc: 0.9992\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.3615e-07 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9992\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.3094e-07 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9992TA: 0s - loss: 1.3116e-07 - acc: 1\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 1.2727e-07 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 0.9992\n",
      "41\n",
      "Train on 14573 samples, validate on 3644 samples\n",
      "Epoch 1/20\n",
      "14573/14573 [==============================] - 18s 1ms/step - loss: 0.0228 - acc: 0.9943 - val_loss: 0.0221 - val_acc: 0.9953\n",
      "Epoch 2/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0070 - acc: 0.9985 - val_loss: 0.0028 - val_acc: 0.9992\n",
      "Epoch 3/20\n",
      "14573/14573 [==============================] - 14s 973us/step - loss: 0.0038 - acc: 0.9995 - val_loss: 0.0080 - val_acc: 0.9973\n",
      "Epoch 4/20\n",
      "14573/14573 [==============================] - 14s 936us/step - loss: 0.0066 - acc: 0.9984 - val_loss: 0.0059 - val_acc: 0.9995\n",
      "Epoch 5/20\n",
      "14573/14573 [==============================] - 13s 923us/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0089 - val_acc: 0.9978\n",
      "Epoch 6/20\n",
      "14573/14573 [==============================] - 14s 929us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0109 - val_acc: 0.9986\n",
      "Epoch 7/20\n",
      "14573/14573 [==============================] - 14s 952us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0105 - val_acc: 0.9986\n",
      "Epoch 8/20\n",
      "14573/14573 [==============================] - 14s 928us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0105 - val_acc: 0.9986\n",
      "Epoch 9/20\n",
      "14573/14573 [==============================] - 14s 930us/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0106 - val_acc: 0.9986\n",
      "Epoch 10/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0107 - val_acc: 0.9986\n",
      "Epoch 11/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0108 - val_acc: 0.9986\n",
      "Epoch 12/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0110 - val_acc: 0.9986\n",
      "Epoch 13/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0111 - val_acc: 0.9986\n",
      "Epoch 14/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0114 - val_acc: 0.9986 ETA\n",
      "Epoch 15/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0116 - val_acc: 0.9986\n",
      "Epoch 16/20\n",
      "14573/14573 [==============================] - 16s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0117 - val_acc: 0.9986\n",
      "Epoch 17/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0119 - val_acc: 0.9986\n",
      "Epoch 18/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0121 - val_acc: 0.9986 - ET\n",
      "Epoch 19/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0123 - val_acc: 0.9986\n",
      "Epoch 20/20\n",
      "14573/14573 [==============================] - 15s 1ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0125 - val_acc: 0.9986\n"
     ]
    }
   ],
   "source": [
    "nn_models = [] # list to keep all models\n",
    "input_n_cols=X_train.shape[1]\n",
    "\n",
    "### for each model\n",
    "for label in unique_words:\n",
    "    \n",
    "#     X_out=X_t_data[int(label)-1]\n",
    "#     y_out=y_t_data[int(label)-1]\n",
    "\n",
    "    \n",
    "\n",
    "    # generate traget lables make others zero: one vs all\n",
    "    target = np.zeros((len(y_train),2),dtype=int)\n",
    "    for i,l_i in enumerate(y_train):\n",
    "        if(int(l_i)==int(label)):\n",
    "            target[i][0]=1\n",
    "        else: \n",
    "            target[i][1]=1\n",
    "    \n",
    "    print(int(label))\n",
    "        \n",
    "    # train model\n",
    "    nn_trainer = NNTrainer(input_n_cols=input_n_cols)\n",
    "    nn_trainer.train(X_train, target)\n",
    "    \n",
    "    # append model\n",
    "    nn_models.append((nn_trainer, label))\n",
    "    nn_trainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATE MODELS\n",
    "\n",
    "Given each test data, we run all the models on it data and pick the one with the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model with best score\n",
    "\n",
    "# as undersample data is marginal reduce the number of test data\n",
    "\n",
    "logprob_1 = np.array([m[0].predict_probability(X_test)[:,0] for m in nn_models]) ## prob of one gorup\n",
    "logprob_2 = np.array([m[0].predict_probability(X_test)[:,1] for m in nn_models]) ## prob of others group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_label = np.argmax(logprob_1, axis=0)\n",
    "# error = (predicted_label != y_test)\n",
    "# print('Overall test accuracy: %.2f percent' % (100 * (1-np.mean(error))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob_1=logprob_1.T\n",
    "logprob_2=logprob_2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall unseen test accuracy: 97.39 percent\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "how to evaluate evaluate model:\n",
    "    select the ones with larget prob.\n",
    "    if prob of ones are euqal for any model then select the model with min others prob\n",
    "'''\n",
    " \n",
    "result=np.zeros((y_test.shape[0]), dtype=int)\n",
    "\n",
    "for i in range(y_test.shape[0]):\n",
    "    max_array=[]\n",
    "    max_n=-100\n",
    "    idx=1\n",
    "    max_min=100\n",
    "    for j in range(41):\n",
    "        max_array.append(logprob_1[i][j])\n",
    "        \n",
    "        if logprob_1[i][j]>max_n and logprob_1[i][j]>=.9:\n",
    "            max_n=logprob_1[i][j]\n",
    "            max_min=logprob_2[i][j]\n",
    "            idx=(j+1)   \n",
    "        if logprob_1[i][j]==max_n and logprob_2[i][j]<max_min:\n",
    "            max_n=logprob_1[i][j]\n",
    "            max_min=logprob_2[i][j]\n",
    "            idx=(j+1)\n",
    "            \n",
    "            \n",
    "    \n",
    "    #sort max array\n",
    "    max_array.sort()\n",
    "    \n",
    "    # compare result with the actual labels\n",
    "    if(int(y_test[i])==int(idx) and max_array[-1]-max_array[-2]>=0.50):\n",
    "        result[i]=1\n",
    "        \n",
    "        \n",
    "overall_acc = np.mean(result)*100\n",
    "print('Overall unseen test accuracy: %.2f percent' % overall_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy on unseen test data (one vs. all)**\n",
    "\n",
    "* Normal Accuracy: 98.60\n",
    "\n",
    "* Accuracy where (h1>=0.9): 98.29\n",
    "\n",
    "* Accuracy where (h1>=0.9 and h2-h2>=0.5): 97.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECK GENERALIZATION ON DIFFERENT MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "in order to check generalization of model, we'll test data 40(40th word features) on model 1 (1 word's data)\n",
    "'''\n",
    "# random undersampling \n",
    "# X_out, y_out = ronadom_undersample(X_train, y_train, int(1))\n",
    "\n",
    "# oversmapling smote\n",
    "# X_out, y_out = smote_sampling(X_train, y_train, int(1))\n",
    "\n",
    "# oversampling multiply\n",
    "X_out, y_out = multiply_oversmaple(X_train, y_train, int(1))\n",
    "\n",
    "# shullfe data\n",
    "X_out, y_out = shuffle(X_out, y_out, random_state=0)\n",
    "\n",
    "target = np.zeros((len(y_out),2),dtype=int)\n",
    "\n",
    "for i,l_i in enumerate(y_out):\n",
    "    if(int(l_i)==40):\n",
    "        target[i][0]=1\n",
    "    else:         \n",
    "        target[i][1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.00%\n"
     ]
    }
   ],
   "source": [
    "model_1_result_41=nn_models[0][0].model_evaluate(X_out,target)\n",
    "print(\"%.2f%%\" % (model_1_result_41))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seems not-targeted data on target model outputs a result with a marginal degree of certainty, 49.00%. This indicates that models trained can make good generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
